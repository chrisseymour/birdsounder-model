{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv to pandas\n",
    "import pandas as pd\n",
    "\n",
    "#previous files 'bird_train.csv' 'bird_train_test.csv' 'brid_train (copy).csv'\n",
    "filename =  'bird_train_2_6.csv' #'bird_train_2_6.csv_all'#\n",
    "filename = '/'.join( ('feature_tables',  filename) )\n",
    "data = pd.read_csv(filename)\n",
    "data.columns = [dd.strip('\\n').strip() for dd in data.columns]\n",
    "#create a copy to chop off the target values\n",
    "data_copy = data.copy(deep=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##drop unimportant features to see if performance improves...\n",
    "#df = data.drop( columns=['num_samples', 'sample_rate', 'species'] )#, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rempove certain species from the dataframe\n",
    "df = data.copy()\n",
    "rem_spec = 'Bald Eagle'\n",
    "data = df.drop(df[df['species'] == rem_spec].sample(frac=1.0).index)\n",
    "#data = df.drop(df[df['species'] == 'Mallard'].sample(frac=1.0).index)\n",
    "data.species.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for the '2_6_all' training table, there are some vastly underrepresended sepcies\n",
    "###to deal with this, remove all spcies under a certain count\n",
    "thresh = 20 #remove species with less than 'thresh' samples\n",
    "vc = data.species.value_counts()# < 10\n",
    "to_rem = vc[ vc <100 ].index\n",
    "new_data = data[ ~data.species.isin( to_rem )]\n",
    "new_data.shape, to_rem, new_data.species.value_counts().shape\n",
    "data = new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.species.value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "ax = data.species.value_counts().plot(kind='bar')\n",
    "if 'all' in filename:\n",
    "    fontsize = 6\n",
    "else:\n",
    "    fontsize=12\n",
    "ax.xaxis.set_ticklabels(ax.xaxis.get_ticklabels(), rotation=70, ha='right', fontsize=fontsize )\n",
    "ax.set_ylabel('number of files', fontsize=12)\n",
    "ax.set_title('Clip Counts per Species', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Begins here:\n",
    "- split the targets and the features\n",
    "- perform one hot encoding, for other model comparison\n",
    "- use a Gradient Boosting Classifier descision tree model\n",
    "- plot the confusion matrix\n",
    "- hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use OneHot encoding to transform categorical data into something useful\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#simple imputer will handle missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "#chain together multiple transformations in one custom filter\n",
    "from sklearn.pipeline import Pipeline\n",
    "#select (by column header/key) which columns get which kind of transformation\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose which columns get transformed\n",
    "cat_cols = ['species']\n",
    "#create the pipeline\n",
    "cat_si_step = ('si', SimpleImputer(strategy='constant',\n",
    "                   fill_value='MISSING'))\n",
    "cat_ohe_step = ('ohe', OneHotEncoder(sparse=False,\n",
    "                    handle_unknown='ignore'))\n",
    "\n",
    "#combine the two transformations into a single Pipeline\n",
    "cat_steps = [cat_si_step, cat_ohe_step]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "\n",
    "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
    "ct = ColumnTransformer(transformers=cat_transformers)\n",
    "\n",
    "target_fit_transformed = ct.fit_transform(data)\n",
    "target_transformed = ct.transform(data) #fit shouldnt matter for one-hot encoding\n",
    "\n",
    "pd.DataFrame( target_fit_transformed ).tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the species column from the test dataframe\n",
    "species_col = data.pop('species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the feature names from the transformation\n",
    "all_steps = ct.named_transformers_['cat']\n",
    "ohe = all_steps.named_steps['ohe']\n",
    "cat_feature_names = ohe.get_feature_names()\n",
    "#clean up the feature names to make more readable\n",
    "ohe_column_names = [cfn.strip('x0_').strip() for cfn in cat_feature_names]\n",
    "cat_feature_names, ohe_column_names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use stratified random sample for the test/train split\n",
    "this will preserve species ratios of the data in train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the resulting model parameters with joblib\n",
    "from joblib import dump, load\n",
    "#dump(cbg, 'cbg_model_100samples.joblib') \n",
    "#load the model for testing\n",
    "#cbg_loaded = load('cbg_model_150samples.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score comparifon funtion\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def GetF1Scores( y_test, y_pred ):\n",
    "    '''\n",
    "    f1 = 2*prec*recall/(prec+recall)\n",
    "    marco is the average f1 score across all species\n",
    "    weighted is the weighted average of all f1 \n",
    "        (i.e. taking the support number for each class into account)\n",
    "    micro is the f1 computed with micro averaged prec and recall\n",
    "        micro prec+recall are treating combining the results from all classes\n",
    "    f1macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1none = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "    return f1macro, f1micro, f1weighted#, f1none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string target\n",
    "import numpy as np\n",
    "\n",
    "X,y = np.array(data), np.array(species_col)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#recalls\n",
    "params_noBE = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 4, 'n_estimators': 125} #best params for 2_6 no bald\n",
    "params_recall_2_6 = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 4, 'n_estimators': 150}\n",
    "params_prec = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 4, 'n_estimators': 200} #prec hyper-tuned for v2 9_species\n",
    "params = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 6, 'n_estimators': 150} #params hyper-tuned for v2 (9 birds 14 features)\n",
    "cbg = GradientBoostingClassifier( )#**params_noBE )# **params_recall_2_6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different under-sample resampling techniques\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\n",
    "##near miss under-sample\n",
    "nm1 = NearMiss(version=1)\n",
    "X_resampled, y_resampled = nm1.fit_resample(X, y)\n",
    "##centroid cluster under-sample\n",
    "#cc = ClusterCentroids(random_state=0)\n",
    "#X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "##random under-sample\n",
    "#rus = RandomUnderSampler(random_state=0)\n",
    "#X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "pd.DataFrame( y_resampled )[0].value_counts(), pd.DataFrame(y)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different under-sample resampling techniques\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "##random over-sample (contains repeats...)\n",
    "#ros = RandomOverSampler(random_state=0)\n",
    "#X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "##SMOTE -- Synthetic Minority Oversampling Technique \n",
    "#X_resampled, y_resampled = SMOTE().fit_resample(X, y) #0.87...need to verify\n",
    "##ADASYN -- Adaptive Synthetic sampling method\n",
    "X_resampled, y_resampled = ADASYN().fit_resample(X, y) #0.834...need to verify\n",
    "\n",
    "#pd.DataFrame( y_resampled )[0].value_counts(), pd.DataFrame(y)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loop through a series of splits to score the model and check the consistency across splits\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.7, random_state=700) #145 userd for <200 #122 for <100 and<300\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ds = []\n",
    "outdict = True\n",
    "\n",
    "X, y = X_resampled, y_resampled\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    cbg.fit(X_train, y_train)\n",
    "    y_pred = cbg.predict( X_test )\n",
    "    #confusion_matrix = MakeConfusionMatrix( y_test, y_pred )\n",
    "    #PrintConfusionMatrix(confusion_matrix.values, confusion_matrix.columns, normalize=True);\n",
    "    print(GetF1Scores( y_test, y_pred ) )\n",
    "    CR = classification_report(y_test, y_pred, output_dict=outdict )\n",
    "    if not outdict:\n",
    "        print( CR )\n",
    "    ds.append( CR )#classification_report(y_test, y_pred))#, output_dict=True ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.5741902466703128, 0.6382155225096761, 0.6302993684382815)\n",
    "(0.5828416983016473, 0.6451415766958647, 0.637782821420117)\n",
    "(0.5486738365971205, 0.6180484823793033, 0.6064974813980757)\n",
    "\n",
    "#with tuned hyper papams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.6036591179305091, 0.6498268486453452, 0.6428665567363259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSplitPreds( X, y, model, nsplits=1, testsize=0.7, random=122):\n",
    "    reports = []\n",
    "    sss = StratifiedShuffleSplit(n_splits=nsplits, test_size=testsize, random_state=random)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #cbg.fit(X_train, y_train)\n",
    "        y_pred = model.predict( X_test )\n",
    "        CR = classification_report(y_test, y_pred, output_dict=outdict )\n",
    "        if not outdict:\n",
    "            print( CR )\n",
    "        ds.append( CR )\n",
    "    return y_test, y_pred, reports\n",
    "\n",
    "y_test, y_pred, reports = GetSplitPreds( X, y, cbg)#_loaded )\n",
    "GetF1Scores( y_test, y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model testing/verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how the model works...be able to explain\n",
    "#grid based hyper parameter searach\n",
    "#sklearn paramater search...\n",
    "#\"I did the parameter tuning\"\n",
    "\n",
    "#cbg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cbg_loaded.predict(X_test)\n",
    "cbg_loaded.score(X_test, y_test)\n",
    "#print( r2_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if they ask about what I would do next, check feature importance with recursive feature elimination\n",
    "to eliminate features and speed up processing time, especially for GridSearchCV hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-801e8165b0ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the results from the loop of splits\n",
    "\n",
    "def GetReportResults( ds ):\n",
    "    accuracies = [d['accuracy'] for d in ds]\n",
    "    #macro scores\n",
    "    macro_pres = [d['macro avg']['precision'] for d in ds]\n",
    "    macro_recall = [d['macro avg']['recall'] for d in ds]\n",
    "    macro_f1 = [d['macro avg']['f1-score'] for d in ds]\n",
    "    macro_support = [d['macro avg']['support'] for d in ds]\n",
    "    #weighted scores\n",
    "    weighted_pres = [d['weighted avg']['precision'] for d in ds]\n",
    "    weighted_recall = [d['weighted avg']['recall'] for d in ds]\n",
    "    weighted_f1 = [d['weighted avg']['f1-score'] for d in ds]\n",
    "    weighted_support = [d['weighted avg']['support'] for d in ds]\n",
    "    print('avg accuracy: {:.3f}'.format( np.average(accuracies) ) )\n",
    "    print('macro')\n",
    "    print(' avg_prescision avg_recall avg_macro')\n",
    "    print('{:.3} {:.3} {:.3}'.format( np.average(macro_pres), np.average(macro_recall), np.average(macro_f1)))\n",
    "    print('weighted')\n",
    "    print( '{:.3} {:.3} {:.3}'.format( np.average(weighted_pres), np.average(weighted_recall), np.average(weighted_f1)))\n",
    "\n",
    "GetReportResults( ds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 scores for default values\n",
    "#they are actually worse...change the hyperparameter tuning properties...\n",
    "GetF1Scores( y_test, y_pred )\n",
    "y_pred.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "def MakeConfusionMatrix( y_test, y_pred ):\n",
    "    data = {'y_Actual':   y_test,\n",
    "            'y_Predicted': y_pred\n",
    "            }\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "    \n",
    "    print( confusion_matrix.shape )\n",
    "    confusion_matrix = confusion_matrix[:-1]\n",
    "    print( confusion_matrix.shape )\n",
    "    confusion_matrix.drop(columns=['All'], inplace=True)\n",
    "    \n",
    "\n",
    "    ##plt.figure(figsize=(14,10))\n",
    "    ##sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    '''disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                     display_labels=class_names,\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     normalize=True)'''\n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "    #b, t = plt.ylim() # discover the values for bottom and top\n",
    "    #b += 0.5 # Add 0.5 to the bottom\n",
    "    #t -= 0.5 # Subtract 0.5 from the top\n",
    "    ##plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    ##plt.show() # ta-da!\n",
    "    return confusion_matrix\n",
    "\n",
    "confusion_matrix = MakeConfusionMatrix( y_test, y_pred )\n",
    "confusion_matrix.columns.shape\n",
    "#confusion_matrix.values, confusion_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#modified from https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, normalize=False, figsize = (10,7), fontsize=6):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        #print(confusion_matrix)\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    \n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    numbers = False\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=numbers, fmt=fmt, cmap='Blues', square=True, xticklabels=True, yticklabels=True)#, cbar_kws={'label': 'accuracy'})\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=70, ha='right', fontsize=fontsize)\n",
    "    heatmap.yaxis.set_label_position('right')\n",
    "    heatmap.xaxis.set_label_position('top')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    return fig\n",
    "\n",
    "confusion_matrix = MakeConfusionMatrix( y_test, y_pred )\n",
    "print_confusion_matrix(confusion_matrix.values, confusion_matrix.columns, normalize=True);\n",
    "print_confusion_matrix(confusion_matrix.values, confusion_matrix.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the entire feature importance bar graph.\n",
    "dfin = []\n",
    "for feat,imp in sorted( zip(data.columns, cbg.feature_importances_), key=lambda l:l[1]):#, reverse=True):\n",
    "    #zxprint(feat.strip(), '{:.3f}'.format(imp) )\n",
    "    dfin.append( (feat.strip(), '{:.3f}'.format(imp)) )\n",
    "df = pd.DataFrame(dfin)#.drop(np.arange(4,40))\n",
    "df.columns = 'features', 'importance'\n",
    "#df.set_index('features', drop=True, inplace=True)\n",
    "df.importance = df.importance.astype(float)\n",
    "f,ax = plt.subplots( figsize=(10,16) )\n",
    "ax.get_xaxis().set_ticks_position('both')\n",
    "#ax.get_xaxis().\n",
    "df.plot.barh(x='features', y='importance', ax=ax ).legend(bbox_to_anchor=(0.95, 0.075))\n",
    "ax.set_title( 'Feature Importance' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://librosa.github.io/librosa/generated/librosa.feature.spectral_rolloff.html\n",
    "The roll-off frequency is defined for each frame as the center frequency for a spectrogram bin such that at least roll_percent (0.85 by default) of the energy of the spectrum in this frame is contained in this bin and the bins below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only plot the top few features...\n",
    "dfin = []\n",
    "i=0\n",
    "for feat,imp in sorted( zip(data.columns, cbg.feature_importances_), key=lambda l:l[1]):#, reverse=True):\n",
    "    #zxprint(feat.strip(), '{:.3f}'.format(imp) )\n",
    "    dfin.append( (feat.strip(), '{:.3f}'.format(imp)) )\n",
    "    #if i==4: break\n",
    "    #i+=1\n",
    "    \n",
    "df = pd.DataFrame(dfin)[-10:]#.drop(np.arange(4,40))\n",
    "df.columns = 'features', 'importance'\n",
    "#df.set_index('features', drop=True, inplace=True)/\n",
    "df.importance = df.importance.astype(float)\n",
    "f,ax = plt.subplots()#figsize=(10,16) )\n",
    "\n",
    "df.plot.barh(x='features', y='importance', ax=ax ).legend(bbox_to_anchor=(0.76, 0.15))\n",
    "#ax.set_yticklabels(reversed( ('Prominent Freq. at Peak Volume', 'Std Deviation Contrast in Band 6', 'Mean Contrast in Band 4', 'Std Deviation Contrast in Band 6', 'Max Contrast Band 6', 'Std Deviation Contrast in Band 5' ) ), fontsize=12)\n",
    "#corresponding = 'ampmax_0 std_cont6 mean_cont4 std_cont4 max_cont6 std_cont5'.split()\n",
    "ax.set_ylabel('importance', fontsize=14)\n",
    "ax.set_xlabel('importance', fontsize=14)\n",
    "ax.set_title( 'Feature Importance', fontsize=18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = 'prominent frequency', 'prominent Q-power frequency', 'prominent mel-frequency'\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hyperparameter tuning for gradient boost classifier\n",
    "#modified from  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "tuned_parameters = {\n",
    "    \"loss\":[\"deviance\"], #, \"exponential\" requires 2 target classes (not-multi...)\n",
    "    \"learning_rate\": [0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    #\"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    #\"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[2,3,4,5],\n",
    "    #\"max_features\":[\"log2\",\"sqrt\"],\n",
    "    #\"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    #\"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[ 25, 50, 100, 125, 150]\n",
    "    }\n",
    "\n",
    "#score on prescision and recall...\n",
    "#clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1)\n",
    "\n",
    "scores = ['f1']# ['recall']\n",
    "weight = 'weighted'\n",
    "output = False\n",
    "\n",
    "for score in scores:\n",
    "    print(f'# Tuning hyper-parameters for {score}\\n')\n",
    "    #using macro scoring...try weighted after? ...for micro, prescision=acuracy...for multiclass\n",
    "    clf = GridSearchCV(GradientBoostingClassifier(), tuned_parameters,\n",
    "                       scoring=f'{score}_{weight}', cv=10, n_jobs=-1)\n",
    "    #clf = GridSearchCV( svm.SVC(), tuned_parameters, scoring=f'{score}_macro' )\n",
    "    clf.fit( X_train, y_train )\n",
    "    print('best params found on development set\\n')\n",
    "    print( clf.best_params_ )\n",
    "    if output:\n",
    "        print('\\ngrid scores on development set:')\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean,std,params in zip( means, stds, clf.cv_results_['params'] ):\n",
    "            print('{:.3f} +/-{:.3f} for {}'.format( mean, 2*std, params) )\n",
    "            print('classification report:\\n')\n",
    "            y_true, y_pred = y_test, clf.predict( X_test )\n",
    "            print( classification_report(y_true, y_pred ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat,imp in zip(data.columns, regr_rf.feature_importances_):\n",
    "    print(feat.strip(), imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regr_rf.fit(X_train, y_train)\n",
    "#evaluate the fit accuracy\n",
    "regr_rf.score(X_test, y_test)\n",
    "#y_pred = regr_rf.predict(X_test)#, y_test)#, multioutput='uniform_average')\n",
    "\n",
    "#r2_score(y_test, y_pred) #...          multioutput='variance_weighted')\n",
    "#y_pred, y_pred.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "sss = StratifiedShuffleSplit(n_splits=4, test_size=0.7, random_state=42)\n",
    "\n",
    "#one hot encoding target\n",
    "X,y = np.array(data), np.array(target_transformed)\n",
    "#string target\n",
    "#X,y = np.array(data), np.array(species_col)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in sss.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #fit the model with the new vaues\n",
    "    regr_rf.fit(X_train, y_train)\n",
    "    #evaluate the fit accuracy\n",
    "    #print(regr_rf.score(X_test, y_test))\n",
    "    y_pred = regr_rf.predict(X_test)#, y_test)#, multioutput='uniform_average')\n",
    "    print( r2_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20\n",
    "for i in range(20):\n",
    "    single = X_test[i].reshape(1,-1) \n",
    "    single_ans = y_test[i]\n",
    "\n",
    "    ab = cbg.predict( single )\n",
    "    probas = cbg.predict_proba( single )\n",
    "    #log_probas = cbg.predict_log_proba( single )\n",
    "\n",
    "    top_results = sorted( zip( cbg.classes_, probas[0] ), key=lambda l: l[1], reverse=True )\n",
    "\n",
    "\n",
    "    print( f'actual {y_test[i]}, predicted {ab}, top_proba=predicted? {ab[0]==top_results[0][0]}' )\n",
    "    for res in zip(  top_results[:3]  ):\n",
    "        print(res)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best params found on development set\n",
    "\n",
    "best_params = {'criterion': 'friedman_mse', 'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 0.17272727272727273, 'n_estimators': 10, 'subsample': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "selector = RFE(estimator, 5, step=1)\n",
    "selector = selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare default with best params found from hyper paramter tuning\n",
    "cbg = GradientBoostingClassifier( **best_params )\n",
    "cbg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cbg.predict(X_test)\n",
    "cbg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sensitivity and specificity, prescision recall, -> short blub about what it says __ talk about two that people ususally use...how well it can predict.  how good is it at false discovery \n",
    "\n",
    "roc curve, what youd like to see goes stainght up then stairght down\n",
    "\n",
    "get rid of bottom row...\n",
    "\n",
    "reason of why XGboost...why did i choose it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the model parameters to feed into the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the resulting model parameters with joblib\n",
    "from joblib import dump, load\n",
    "dump(cbg, 'cbg_model_300samples.joblib') \n",
    "#test the loading of the model to verify it's functionality\n",
    "#cbg_loaded = load('cbg_model_all.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cbg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cbg_loaded.predict(X_test)\n",
    "cbg_loaded.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print( classification_report(y_true, y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(decision_function_shape='ovo', gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tuned_parameters = [ {'kernel': ['rbf'], 'gamma': [1e-3],# 1e-4],\n",
    "                      'C': [1, 10]},#, 100, 1000]},\n",
    "                     {'kernel': ['linear'], 'C': [1, 10]}]#, 100, 1000]} ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(f'# Tuning hyper-parameters for {score}\\n')\n",
    "    \n",
    "    clf = GridSearchCV(\n",
    "        svm.SVC(), tuned_parameters, scoring=f'{score}_macro', n_jobs=-1 )\n",
    "    clf.fit( X_train, y_train )\n",
    "    print('best params found on development set\\n')\n",
    "    print( clf.best_params_ )\n",
    "    print('\\ngrid scores on development set:\\n')\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean,std,params in zip( means, stds, clf.cv_results_['params'] ):\n",
    "        print('{:.3f} +/-{:.3f} for {}'.format( mean, 2*std, params) )\n",
    "        print('classification report:\\n')\n",
    "        y_true, y_pred = y_test, clf.predict( X_test )\n",
    "        print( classification_report(y_true, y_pred ) )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reciever operator curve for multi class classifier\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Import some data to play with\n",
    "##iris = datasets.load_iris()\n",
    "##X = iris.data\n",
    "##y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "##y = label_binarize(y, classes=[0, 1, 2])\n",
    "y = label_binarize(y, classes=ohe_column_names )\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(42)\n",
    "#n_samples, n_features = X.shape\n",
    "#X = np.c_[X, random_state.randn(n_samples, 20 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "##X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.7, random_state=random_state)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "#classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "classifier = OneVsRestClassifier(GradientBoostingClassifier( random_state=random_state),n_jobs=-1)\n",
    "                                 #random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "\n",
    "##colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "colors = cycle(['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink',\n",
    "                'tab:gray', 'tab:olive', 'tab:cyan'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.title('Receiver Operating Characteristic Curves')\n",
    "plt.legend(loc=\"lower right\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean,std,params in zip( means, stds, clf.cv_results_['params'] ):\n",
    "        print('{:.3f} +/-{:.3f} for {}'.format( mean, 2*std, params) )\n",
    "        print('classification report:\\n')\n",
    "        y_true, y_pred = y_test, clf.predict( X_test )\n",
    "        print( classification_report(y_true, y_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take the ratio for the 'all' row and column for each species, to determine the percentage of each species that was predicted corretly\n",
    "- if it is over 1, then it was 'over-fitted' to that species (i.e. it predicted that species more that there actually were'\n",
    "- if it is less, then (i would think) this is better since it just mis-identified things and not thought everything was just \"a blue jay\" because the data set was mostly bue jays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tn, tp are diagonals\n",
    "#fp = top predicted true, but actually false\n",
    "#fn = predicted false, but actually true\n",
    "#tn,fp,fn,tp = confusion_matrix.ravel()\n",
    "CM = np.array(confusion_matrix)\n",
    "tnfp = CM.ravel()\n",
    "tp = 0\n",
    "n = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "print(CM.shape)\n",
    "for i,row in enumerate(CM):\n",
    "    for j,col in enumerate(row):\n",
    "        n += col\n",
    "        #print(i,j, CM[i][j], col)\n",
    "        if i==j:\n",
    "            tp += col #CM[i][j]\n",
    "        if i>j:\n",
    "            fn += col\n",
    "        if i<j: fp += col\n",
    "\n",
    "            \n",
    "#confusion_matrix.all\n",
    "tp, n, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = multilabel_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "#multilabel_confusion_matrix(y_test, y_predicted)  #give error due to continous output of RF regressor\n",
    "#y_pred = (y_pred > 0.5) \n",
    "CM = multilabel_confusion_matrix(y_test, y_pred)\n",
    "#CM = confusion_matrix(y_test, y_pred)\n",
    "#CMml, CMml.shape\n",
    "plt.figure()\n",
    "sns.heatmap(CM[:,:,1], annot=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.heatmap(CM[:,:,0], annot=True)\n",
    "plt.tight_layout()\n",
    "CM.shape, CM[:,:,0]\n",
    "#sn.heatmap(CM, annot=True)\n",
    "pd.DataFrame(y_pred).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=1000, max_depth=max_depth, random_state=12))\n",
    "regr_multirf.fit(X_train, y_train)\n",
    "regr_multirf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_one = ('zero', 'one')\n",
    "label_two = ohe_column_names\n",
    "cols = pd.MultiIndex.from_product([label_one, label_two])\n",
    "\n",
    "pd.DataFrame(CM.T.reshape(2, -1), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "#get the prediction results\n",
    "y_predicted = regr_rf.predict(X_test)\n",
    "#store the actual answers and the predicted answers in a DataFrame\n",
    "prediction_data = {'y_Actual':   y_test,\n",
    "        'y_Predicted': y_predicted }\n",
    "\n",
    "df = pd.DataFrame( y_predicted, y_test )\n",
    "#df = pd.DataFrame(prediction_data)#, columns=['y_Actual','y_Predicted'])\n",
    "#calculate the confusion matrix\n",
    "##confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "\n",
    "#plot the heatmap using seaborn\n",
    "##sn.heatmap(confusion_matrix, annot=True)\n",
    "#df = pd.DataFrame(prediction_data, columns=['y_Actual','y_Predicted'])\n",
    "#y_test\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio of species in train vs test (ideally should be ~3 for all)\n",
    "train, test = pd.DataFrame( y_train ), pd.DataFrame( y_test )\n",
    "train.columns, test.columns = ['species'], ['species']\n",
    "train.species.value_counts()/test.species.value_counts(), test.species.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = regr_rf.predict(X_test)\n",
    "for i,prediction in enumerate(predicted):\n",
    "    print(i, y_test[i] - prediction)\n",
    "    #print( sum(y_test[i] - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "a = ((-1, 1, 2, 'a'),\n",
    "     (3, 4, 2, 'b'),\n",
    "     (6, 7, 3, 'c'),\n",
    "     (9, 10, 3, 'd'))\n",
    "\n",
    "df = pd.DataFrame( a )\n",
    "df.columns = 'first second third letters'.split()\n",
    "g_works = sns.pairplot(df)\n",
    "g_broken = sns.pairplot(df, hue=\"letters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
