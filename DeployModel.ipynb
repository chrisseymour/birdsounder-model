{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv to pandas\n",
    "import pandas as pd\n",
    "\n",
    "#previous files 'bird_train.csv' 'bird_train_test.csv' 'brid_train (copy).csv'\n",
    "filename =  'bird_train_2_6_all.csv' #'bird_train_2_6.csv'#\n",
    "filename = '/'.join( ('feature_tables',  filename) )\n",
    "data = pd.read_csv(filename)\n",
    "data.columns = [dd.strip('\\n').strip() for dd in data.columns]\n",
    "#create a copy to chop off the target values\n",
    "data_copy = data.copy(deep=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##drop unimportant features to see if performance improves...\n",
    "#df = data.drop( columns=['num_samples', 'sample_rate', 'species'] )#, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rempove certain species from the dataframe\n",
    "df = data.copy()\n",
    "rem_spec = 'Bald Eagle'\n",
    "data = df.drop(df[df['species'] == rem_spec].sample(frac=1.0).index)\n",
    "#data = df.drop(df[df['species'] == 'Mallard'].sample(frac=1.0).index)\n",
    "data.species.value_counts()#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for the '2_6_all' training table, there are some vastly underrepresended sepcies\n",
    "###to deal with this, remove all spcies under a certain count\n",
    "thresh = 20 #remove species with less than 'thresh' samples\n",
    "vc = data.species.value_counts()# < 10\n",
    "to_rem = vc[ vc <100 ].index\n",
    "new_data = data[ ~data.species.isin( to_rem )]\n",
    "new_data.shape, to_rem, new_data.species.value_counts().shape\n",
    "data = new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, v in zip(data.species.value_counts().index, data.species.value_counts()):#.shape\n",
    "    print(p, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "ax = data.species.value_counts().plot(kind='bar')\n",
    "if 'all' in filename:\n",
    "    fontsize = 6\n",
    "else:\n",
    "    fontsize=12\n",
    "ax.xaxis.set_ticklabels(ax.xaxis.get_ticklabels(), rotation=70, ha='right', fontsize=fontsize )\n",
    "ax.set_ylabel('number of files', fontsize=12)\n",
    "ax.set_title('Clip Counts per Species', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Begins here:\n",
    "- split the targets and the features\n",
    "- perform one hot encoding, for other model comparison\n",
    "- use a Gradient Boosting Classifier descision tree model\n",
    "- plot the confusion matrix\n",
    "- hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use OneHot encoding to transform categorical data into something useful\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#simple imputer will handle missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "#chain together multiple transformations in one custom filter\n",
    "from sklearn.pipeline import Pipeline\n",
    "#select (by column header/key) which columns get which kind of transformation\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose which columns get transformed\n",
    "cat_cols = ['species']\n",
    "#create the pipeline\n",
    "cat_si_step = ('si', SimpleImputer(strategy='constant',\n",
    "                   fill_value='MISSING'))\n",
    "cat_ohe_step = ('ohe', OneHotEncoder(sparse=False,\n",
    "                    handle_unknown='ignore'))\n",
    "\n",
    "#combine the two transformations into a single Pipeline\n",
    "cat_steps = [cat_si_step, cat_ohe_step]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "\n",
    "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
    "ct = ColumnTransformer(transformers=cat_transformers)\n",
    "\n",
    "target_fit_transformed = ct.fit_transform(data)\n",
    "target_transformed = ct.transform(data) #fit shouldnt matter for one-hot encoding\n",
    "\n",
    "pd.DataFrame( target_fit_transformed ).tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the species column from the test dataframe\n",
    "species_col = data.pop('species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the feature names from the transformation\n",
    "all_steps = ct.named_transformers_['cat']\n",
    "ohe = all_steps.named_steps['ohe']\n",
    "cat_feature_names = ohe.get_feature_names()\n",
    "#clean up the feature names to make more readable\n",
    "ohe_column_names = [cfn.strip('x0_').strip() for cfn in cat_feature_names]\n",
    "cat_feature_names, ohe_column_names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use stratified random sample for the test/train split\n",
    "this will preserve species ratios of the data in train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the resulting model parameters with joblib\n",
    "from joblib import dump, load\n",
    "#dump(cbg, 'cbg_model_100samples.joblib') \n",
    "#load the model for testing\n",
    "#cbg_loaded = load('cbg_model_150samples.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score comparifon funtion\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def GetF1Scores( y_test, y_pred ):\n",
    "    '''\n",
    "    f1 = 2*prec*recall/(prec+recall)\n",
    "    marco is the average f1 score across all species\n",
    "    weighted is the weighted average of all f1 \n",
    "        (i.e. taking the support number for each class into account)\n",
    "    micro is the f1 computed with micro averaged prec and recall\n",
    "        micro prec+recall are treating combining the results from all classes\n",
    "    '''\n",
    "    f1macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1none = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "    return f1macro, f1micro, f1weighted#, f1none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string target\n",
    "import numpy as np\n",
    "\n",
    "X,y = np.array(data), np.array(species_col)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#recalls\n",
    "params_noBE = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 4, 'n_estimators': 125} #best params for 2_6 no bald\n",
    "params_recall_2_6 = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 4, 'n_estimators': 150}\n",
    "params_prec = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 4, 'n_estimators': 200} #prec hyper-tuned for v2 9_species\n",
    "params = {'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 6, 'n_estimators': 150} #params hyper-tuned for v2 (9 birds 14 features)\n",
    "cbg = GradientBoostingClassifier( )#**params_noBE )# **params_recall_2_6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different under-sample resampling techniques\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\n",
    "##near miss under-sample\n",
    "nm1 = NearMiss(version=1)\n",
    "X_resampled, y_resampled = nm1.fit_resample(X, y)\n",
    "##centroid cluster under-sample\n",
    "#cc = ClusterCentroids(random_state=0)\n",
    "#X_resampled, y_resampled = cc.fit_resample(X, y)\n",
    "##random under-sample\n",
    "#rus = RandomUnderSampler(random_state=0)\n",
    "#X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "pd.DataFrame( y_resampled )[0].value_counts(), pd.DataFrame(y)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different under-sample resampling techniques\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "##random over-sample (contains repeats...)\n",
    "#ros = RandomOverSampler(random_state=0)\n",
    "#X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "##SMOTE -- Synthetic Minority Oversampling Technique \n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y) #need to use only on test set\n",
    "##ADASYN -- Adaptive Synthetic sampling method\n",
    "#X_resampled, y_resampled = ADASYN().fit_resample(X, y) #need to use only on test set\n",
    "\n",
    "pd.DataFrame( y_resampled )[0].value_counts(), pd.DataFrame(y)[0].value_counts(), len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loop through a series of splits to score the model and check the consistency across splits\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.7, random_state=700) #145 userd for <200 #122 for <100 and<300\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ds = []\n",
    "outdict = True\n",
    "resample = True\n",
    "\n",
    "\n",
    "X, y = X_resampled, y_resampled\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    if resample:\n",
    "        X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "    \n",
    "    cbg.fit(X_train, y_train)\n",
    "    y_pred = cbg.predict( X_test )\n",
    "    #confusion_matrix = MakeConfusionMatrix( y_test, y_pred )\n",
    "    #PrintConfusionMatrix(confusion_matrix.values, confusion_matrix.columns, normalize=True);\n",
    "    print(GetF1Scores( y_test, y_pred ) )\n",
    "    CR = classification_report(y_test, y_pred, output_dict=outdict )\n",
    "    if not outdict:\n",
    "        print( CR )\n",
    "    ds.append( CR )#classification_report(y_test, y_pred))#, output_dict=True ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.5741902466703128, 0.6382155225096761, 0.6302993684382815)\n",
    "(0.5828416983016473, 0.6451415766958647, 0.637782821420117)\n",
    "(0.5486738365971205, 0.6180484823793033, 0.6064974813980757)\n",
    "\n",
    "#with tuned hyper papams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.6036591179305091, 0.6498268486453452, 0.6428665567363259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSplitPreds( X, y, model, nsplits=1, testsize=0.7, random=122):\n",
    "    reports = []\n",
    "    sss = StratifiedShuffleSplit(n_splits=nsplits, test_size=testsize, random_state=random)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #cbg.fit(X_train, y_train)\n",
    "        y_pred = model.predict( X_test )\n",
    "        CR = classification_report(y_test, y_pred, output_dict=outdict )\n",
    "        if not outdict:\n",
    "            print( CR )\n",
    "        ds.append( CR )\n",
    "    return y_test, y_pred, reports\n",
    "\n",
    "y_test, y_pred, reports = GetSplitPreds( X, y, cbg)#_loaded )\n",
    "GetF1Scores( y_test, y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model testing/verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how the model works...be able to explain\n",
    "#grid based hyper parameter searach\n",
    "#sklearn paramater search...\n",
    "#\"I did the parameter tuning\"\n",
    "\n",
    "#cbg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cbg_loaded.predict(X_test)\n",
    "cbg_loaded.score(X_test, y_test)\n",
    "#print( r2_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if they ask about what I would do next, check feature importance with recursive feature elimination\n",
    "to eliminate features and speed up processing time, especially for GridSearchCV hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the results from the loop of splits\n",
    "\n",
    "def GetReportResults( ds ):\n",
    "    accuracies = [d['accuracy'] for d in ds]\n",
    "    #macro scores\n",
    "    macro_pres = [d['macro avg']['precision'] for d in ds]\n",
    "    macro_recall = [d['macro avg']['recall'] for d in ds]\n",
    "    macro_f1 = [d['macro avg']['f1-score'] for d in ds]\n",
    "    macro_support = [d['macro avg']['support'] for d in ds]\n",
    "    #weighted scores\n",
    "    weighted_pres = [d['weighted avg']['precision'] for d in ds]\n",
    "    weighted_recall = [d['weighted avg']['recall'] for d in ds]\n",
    "    weighted_f1 = [d['weighted avg']['f1-score'] for d in ds]\n",
    "    weighted_support = [d['weighted avg']['support'] for d in ds]\n",
    "    print('avg accuracy: {:.3f}'.format( np.average(accuracies) ) )\n",
    "    print('macro')\n",
    "    print(' avg_prescision avg_recall avg_macro')\n",
    "    print('{:.3} {:.3} {:.3}'.format( np.average(macro_pres), np.average(macro_recall), np.average(macro_f1)))\n",
    "    print('weighted')\n",
    "    print( '{:.3} {:.3} {:.3}'.format( np.average(weighted_pres), np.average(weighted_recall), np.average(weighted_f1)))\n",
    "\n",
    "GetReportResults( ds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 scores for default values\n",
    "#they are actually worse...change the hyperparameter tuning properties...\n",
    "GetF1Scores( y_test, y_pred )\n",
    "y_pred.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "def MakeConfusionMatrix( y_test, y_pred ):\n",
    "    data = {'y_Actual':   y_test,\n",
    "            'y_Predicted': y_pred\n",
    "            }\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "    \n",
    "    print( confusion_matrix.shape )\n",
    "    confusion_matrix = confusion_matrix[:-1]\n",
    "    print( confusion_matrix.shape )\n",
    "    confusion_matrix.drop(columns=['All'], inplace=True)\n",
    "    \n",
    "\n",
    "    ##plt.figure(figsize=(14,10))\n",
    "    ##sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    '''disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                     display_labels=class_names,\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     normalize=True)'''\n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "    #b, t = plt.ylim() # discover the values for bottom and top\n",
    "    #b += 0.5 # Add 0.5 to the bottom\n",
    "    #t -= 0.5 # Subtract 0.5 from the top\n",
    "    ##plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    ##plt.show() # ta-da!\n",
    "    return confusion_matrix\n",
    "\n",
    "confusion_matrix = MakeConfusionMatrix( y_test, y_pred )\n",
    "confusion_matrix.columns.shape\n",
    "#confusion_matrix.values, confusion_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#modified from https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, normalize=False, figsize = (10,7), fontsize=6):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        #print(confusion_matrix)\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    \n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    numbers = False\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=numbers, fmt=fmt, cmap='Blues', square=True, xticklabels=True, yticklabels=True)#, cbar_kws={'label': 'accuracy'})\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=70, ha='right', fontsize=fontsize)\n",
    "    heatmap.yaxis.set_label_position('right')\n",
    "    heatmap.xaxis.set_label_position('top')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    return fig\n",
    "\n",
    "confusion_matrix = MakeConfusionMatrix( y_test, y_pred )\n",
    "print_confusion_matrix(confusion_matrix.values, confusion_matrix.columns, normalize=True);\n",
    "print_confusion_matrix(confusion_matrix.values, confusion_matrix.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the entire feature importance bar graph.\n",
    "dfin = []\n",
    "for feat,imp in sorted( zip(data.columns, cbg.feature_importances_), key=lambda l:l[1]):#, reverse=True):\n",
    "    #zxprint(feat.strip(), '{:.3f}'.format(imp) )\n",
    "    dfin.append( (feat.strip(), '{:.3f}'.format(imp)) )\n",
    "df = pd.DataFrame(dfin)#.drop(np.arange(4,40))\n",
    "df.columns = 'features', 'importance'\n",
    "#df.set_index('features', drop=True, inplace=True)\n",
    "df.importance = df.importance.astype(float)\n",
    "f,ax = plt.subplots( figsize=(10,16) )\n",
    "ax.get_xaxis().set_ticks_position('both')\n",
    "#ax.get_xaxis().\n",
    "df.plot.barh(x='features', y='importance', ax=ax ).legend(bbox_to_anchor=(0.95, 0.075))\n",
    "ax.set_title( 'Feature Importance' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only plot the top few features...\n",
    "dfin = []\n",
    "i=0\n",
    "for feat,imp in sorted( zip(data.columns, cbg.feature_importances_), key=lambda l:l[1]):#, reverse=True):\n",
    "    #zxprint(feat.strip(), '{:.3f}'.format(imp) )\n",
    "    dfin.append( (feat.strip(), '{:.3f}'.format(imp)) )\n",
    "    #if i==4: break\n",
    "    #i+=1\n",
    "    \n",
    "df = pd.DataFrame(dfin)[-10:]#.drop(np.arange(4,40))\n",
    "df.columns = 'features', 'importance'\n",
    "#df.set_index('features', drop=True, inplace=True)/\n",
    "df.importance = df.importance.astype(float)\n",
    "f,ax = plt.subplots()#figsize=(10,16) )\n",
    "\n",
    "df.plot.barh(x='features', y='importance', ax=ax ).legend(bbox_to_anchor=(0.76, 0.15))\n",
    "#ax.set_yticklabels(reversed( ('Prominent Freq. at Peak Volume', 'Std Deviation Contrast in Band 6', 'Mean Contrast in Band 4', 'Std Deviation Contrast in Band 6', 'Max Contrast Band 6', 'Std Deviation Contrast in Band 5' ) ), fontsize=12)\n",
    "#corresponding = 'ampmax_0 std_cont6 mean_cont4 std_cont4 max_cont6 std_cont5'.split()\n",
    "ax.set_ylabel('importance', fontsize=14)\n",
    "ax.set_xlabel('importance', fontsize=14)\n",
    "ax.set_title( 'Feature Importance', fontsize=18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the classes trained by the model\n",
    "#cbg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = 'prominent frequency', 'prominent Q-power frequency', 'prominent mel-frequency'\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hyperparameter tuning for gradient boost classifier\n",
    "#modified from  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "tuned_parameters = {\n",
    "    \"loss\":[\"deviance\"], #, \"exponential\" requires 2 target classes (not-multi...)\n",
    "    \"learning_rate\": [0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    #\"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    #\"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[2,3,4,5],\n",
    "    #\"max_features\":[\"log2\",\"sqrt\"],\n",
    "    #\"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    #\"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[ 25, 50, 100, 125, 150]\n",
    "    }\n",
    "\n",
    "#score on prescision and recall...\n",
    "#clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1)\n",
    "\n",
    "scores = ['f1']# ['recall']\n",
    "weight = 'weighted'\n",
    "output = False\n",
    "\n",
    "for score in scores:\n",
    "    print(f'# Tuning hyper-parameters for {score}\\n')\n",
    "    #using macro scoring...try weighted after? ...for micro, prescision=acuracy...for multiclass\n",
    "    clf = GridSearchCV(GradientBoostingClassifier(), tuned_parameters,\n",
    "                       scoring=f'{score}_{weight}', cv=10, n_jobs=-1)\n",
    "    #clf = GridSearchCV( svm.SVC(), tuned_parameters, scoring=f'{score}_macro' )\n",
    "    clf.fit( X_train, y_train )\n",
    "    print('best params found on development set\\n')\n",
    "    print( clf.best_params_ )\n",
    "    if output:\n",
    "        print('\\ngrid scores on development set:')\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean,std,params in zip( means, stds, clf.cv_results_['params'] ):\n",
    "            print('{:.3f} +/-{:.3f} for {}'.format( mean, 2*std, params) )\n",
    "            print('classification report:\\n')\n",
    "            y_true, y_pred = y_test, clf.predict( X_test )\n",
    "            print( classification_report(y_true, y_pred ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC (Reciever Operator Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reciever operator curve for multi class classifier\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Import some data to play with\n",
    "##iris = datasets.load_iris()\n",
    "##X = iris.data\n",
    "##y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "##y = label_binarize(y, classes=[0, 1, 2])\n",
    "y = label_binarize(y, classes=ohe_column_names )\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(42)\n",
    "#n_samples, n_features = X.shape\n",
    "#X = np.c_[X, random_state.randn(n_samples, 20 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "##X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.7, random_state=random_state)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "#classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "classifier = OneVsRestClassifier(GradientBoostingClassifier( random_state=random_state),n_jobs=-1)\n",
    "                                 #random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "plotall = True\n",
    "if plotall:\n",
    "    ##colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    colors = cycle(['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink',\n",
    "                    'tab:gray', 'tab:olive', 'tab:cyan'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.title('Receiver Operating Characteristic Curves')\n",
    "plt.legend(loc=\"lower right\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
